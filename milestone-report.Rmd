---
title: "Data Science Capstone - Milestone Report"
author: "Stefan Botha"
date: "June 13, 2018"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
library(stringr)
library(tibble)
library(kableExtra)
library(tm)
library(RWeka)
library(SnowballC)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE, fig.align = "center")
options(knitr.table.format = "html") 
```

## 1) Primary intro

This is my intro to the Milestone Report.
 It aims to develop a shiny app that taes the input of a phrase and predict the following word.Thi is done using swiftkey

## Data set description

Data set obtained from helloset repo (archived version obtained


```{r message=FALSE}
# obtain file list
listOfFiles <- dir("HC_Corpora", recursive = TRUE, full.names = TRUE)

# Bullets below
kable(cbind(
          seq(1, length(listOfFiles)), 
          listOfFiles), 
      col.names = c('#', 'File')) %>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)
```
Twitter lines for reference:

 - ***de_DE.twitter.txt***:
```{r}
connnectionBlogsFile <- file("HC_Corpora/de_DE/de_DE.twitter.txt", "r")
readLines(connnectionBlogsFile, 3)
close(connnectionBlogsFile)
```
 - ***en_US.twitter.txt***:
```{r}
connnectionBlogsFile <- file("HC_Corpora/en_US/en_US.twitter.txt", "r")
readLines(connnectionBlogsFile, 3)
close(connnectionBlogsFile)
```
Cleaning stage to follow
 
### 2.2) Dataset details

Words per line rration takes centre stage here

```{r message=FALSE}
# obtain stat list from files all
listOfFileInfos <- data.frame(file = listOfFiles, size = file.info(listOfFiles)$size)
listOfFileInfos$sizeInMB <- round(listOfFileInfos$size / (1024 * 1024), digits = 2)

# 4 column stat generation
listOfFileInfos$lineCount <- 0
listOfFileInfos$wordCount <- 0
listOfFileInfos$wordsPerLineRatio <- 0

# adding a column in order to show the file language
listOfFileInfos <- listOfFileInfos %>%
  rowwise() %>% 
  mutate(language = 
           ifelse(str_detect(file, "en_US"), 'English', 
             ifelse(str_detect(file, "de_DE"), 'German',
               ifelse(str_detect(file, "fi_FI"), 'Finnish',
                 ifelse(str_detect(file, "ru_RU"), 'Russian', 'not-defined')))))

# Auxiliary function. It allows get data from files using the 'wc' command.
executeWc <- function(x) as.numeric(str_split(system(paste0("wc ", x), intern = TRUE),  boundary("word"))[[1]][1:2])

# Complete de file stats with the 'wc' command data
for (index in 1:nrow(listOfFileInfos)) {
  wcCommandResults <- executeWc(listOfFileInfos[index,]$file)
  
  listOfFileInfos[index,]$lineCount <- wcCommandResults[1]
  listOfFileInfos[index,]$wordCount <- wcCommandResults[2]
  listOfFileInfos[index,]$wordsPerLineRatio <- round(wcCommandResults[2] / wcCommandResults[1], digits = 2)
}

columNamesToShow <- c('File', 'Size', 'Size in MB', 'Line count', 'Word count', 'W/L ratio', 'Language')

# Show a formatted table
kable(listOfFileInfos, col.names = columNamesToShow)  %>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)

```

In the context of the Capstone project, only the English language files will be taken into account, that is:

```{r results='asis'}
# Select files in english language
englishFiles <- listOfFileInfos[listOfFileInfos$language == "English",]

# Show a formatted table
kable(englishFiles, col.names = columNamesToShow)%>%
      kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
          full_width = FALSE)


```

### 2.2) Dataset cleaning

Data set is cleaned using multio io stat red

```{r}

tweets <- readLines('HC_Corpora/en_US/en_US.twitter.txt', encoding = 'UTF-8', skipNul = TRUE)
tweets <- iconv(tweets, to = "ASCII", sub="")

blogs <- readLines('HC_Corpora/en_US/en_US.blogs.txt', encoding = 'UTF-8', skipNul = TRUE)

newsFileConnection <- file('HC_Corpora/en_US/en_US.news.txt', encoding = 'UTF-8', open = 'rb')
news <- readLines(newsFileConnection, skipNul = TRUE)
close(newsFileConnection)

sampledText <- c(
  blogs[sample(1:length(blogs),length(blogs)/100)], 
  news[sample(1:length(news),length(news)/100)], 
  tweets[sample(1:length(tweets),length(tweets)/100)])
remove(blogs)
remove(tweets)
remove(news)

```
Txt mining capabilities are readly initiated


```{r build-corpus}

sampledText <- iconv(sampledText, to = "ASCII", sub="")

corpus <- VCorpus(VectorSource(sampledText))
corpus

# Utilitary function, for counting the words in a corpus.
corpusWordCounter <- function(corpus) {
  sum(sapply(corpus, str_count, pattern = "\\S+"))
}

originalWordCount <- corpusWordCounter(corpus)

```
The corpus has ***`r originalWordCount` words***  approximately.

Sample follows:
```{r}
writeLines(as.character(corpus[[1]]))
writeLines(as.character(corpus[[2]]))
```

Transformation below

* *Uniforming the text to lowercase*
* *Removing punctuation, number, special characters, etc.*
* *Striping whitespaces*
* *Removing stop words*
* *Profanity filtering (Removing swear words)*
* *Stemming the text*

Many of these task are performed using ***tm*** transformation operations, the rest of them need certain custom coding, usign the function ***content_transformer()*** from ***tm*** package. The transformations provided by ***tm*** package are:
```{r}
getTransformations()
```

# Special single quotes
corpus <- tm_map(corpus, toSpace, "[\x82\x91\x92]")

# URIs
corpus <- tm_map(corpus, toSpace, '(ftp|http|https)[^([:blank:]|\\"|<|&|#\n\r)]+')
# Twitter users and hashtags
corpus <- tm_map(corpus, toSpace, '(@|#)[^\\s]+')
# Emails addresses
corpus <- tm_map(corpus, toSpace, '^[[:alnum:].-_]+@[[:alnum:].-]+$')

corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)

writeLines(as.character(corpus[[1]]))
writeLines(as.character(corpus[[2]]))

```

#### 2.2.3) Striping whitespaces
In this transformation, multiple whitespaces are collapsed to a single blank. The operation is perfomed using the  ***stripWhitespace()*** transformation:
```{r striping_whitespaces}

corpus <- tm_map(corpus, stripWhitespace)

writeLines(as.character(corpus[[1]]))
writeLines(as.character(corpus[[2]]))

```

#### 2.2.4) Removing stop words
  ***stopwords()*** transformation used:
  
```{r}
corpus <- tm_map(corpus, removeWords, stopwords("english"))

writeLines(as.character(corpus[[1]]))
writeLines(as.character(corpus[[2]]))
```

#### 2.2.5) Profanity filtering
In a sense no more swearing;

```{r removing_swear_words}
swearWordsFileUrl <- 'http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/Terms-to-Block.csv'
rawSwearWords <- readLines(swearWordsFileUrl)
swearWords <- gsub(',"?', '', rawSwearWords[5:length(rawSwearWords)])

sample(swearWords, 10)

corpus <- tm_map(corpus, removeWords, swearWords)

```

#### 2.2.6) Stemming the text
 Root inititiated 
```{r}

corpus <- tm_map(corpus, stemDocument)

lastTransformationWordCount <- corpusWordCounter(corpus)

```
Finally, the corpus has ***`r lastTransformationWordCount` words***, ***`r originalWordCount - lastTransformationWordCount`*** less than from the beginning.

## 3) Analysis
### 3.1) Exploratory Analisis

Calculations described initiated with timeline representation

```{r dtm}

# Tokenizers based on NLP package
unigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))

# Utility function, for getting the top ten frequencies
getNgramFrequencies <- function(dtm) {
  sort(colSums(as.matrix(dtm)), decreasing = TRUE)
}

unigramDtm  <- DocumentTermMatrix(corpus, control = list(tokenize = unigramTokenizer))
unigramDtm <- removeSparseTerms(unigramDtm, 0.999)
unigramFrequencies <- getNgramFrequencies(unigramDtm)
unigram10Frequencies <- unigramFrequencies[1:10]
unigramFrequenciesDF <- data.frame(word = names(unigram10Frequencies), frequency = as.numeric(unigram10Frequencies))

bigramDtm  <- DocumentTermMatrix(corpus, control = list(tokenize = bigramTokenizer))
bigramDtm <- removeSparseTerms(bigramDtm, 0.999)
bigramFrequencies <- getNgramFrequencies(bigramDtm)
bigram10Frequencies <- bigramFrequencies[1:10]
bigramFrequenciesDF <- data.frame(bigram = names(bigram10Frequencies), frequency = as.numeric(bigram10Frequencies))

trigramDtm <- DocumentTermMatrix(corpus, control = list(tokenize = trigramTokenizer))
trigramDtm <- removeSparseTerms(trigramDtm, 0.9999)
trigramFrequencies <- getNgramFrequencies(trigramDtm)
trigram10Frequencies <- trigramFrequencies[1:10]
trigramFrequenciesDF <- data.frame(trigram = names(trigram10Frequencies), frequency = as.numeric(trigram10Frequencies))

```

- ***For words***:
```{r unigrams-details}

kable(unigramFrequenciesDF, col.names = c('Word', 'Frequency'))  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
    full_width = FALSE)

ggplot(data = unigramFrequenciesDF, aes(reorder(word, -frequency), frequency)) +
  geom_bar(stat = "identity") +
  ggtitle("Most frequent words") +
  xlab("Words") + ylab("Frequency") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

- ***For 2-Grams***:
```{r bigrams-details}

kable(bigramFrequenciesDF, col.names = c('2-Gram', 'Frequency'))  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
    full_width = FALSE)

ggplot(data = bigramFrequenciesDF, aes(reorder(bigram, -frequency), frequency)) +
  geom_bar(stat = "identity") +
  ggtitle("Most frequent 2-Grams") +
  xlab("2-Grams") + ylab("Frequency") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

```

- ***For 3-Grams***:
```{r trigrams-details}

kable(trigramFrequenciesDF, col.names = c('Word', 'Frequency'))  %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive", "bordered"), 
    full_width = FALSE)

ggplot(data = trigramFrequenciesDF, aes(reorder(trigram, -frequency), frequency)) +
  geom_bar(stat = "identity") +
  ggtitle("Most frequent 3-Grams") +
  xlab("3-Grams") + ylab("Frequency") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Word frequencies listed

*Word instances:*
```{r}
totalOfWordInstances <- sum(unigramFrequencies)

totalOfWordInstances
```

*Unique words:*
```{r}
totalOfUniqueWords <- length(unigramFrequencies)

totalOfUniqueWords

# Utilitary function. Calculate the amount of unique words for a selected coverage level
findAmountWordsForCoverage <- function(descendingFrequencies, coverage) {
  
  totalOfWordInstances <- sum(descendingFrequencies)
  totalOfUniqueWords <- length(descendingFrequencies)

  coveragePercentage <- totalOfWordInstances * (coverage  / 100)
  accumulatedWords <- 0
  lastIndex <- 0
  
  for (index in seq_len(totalOfUniqueWords)) { 
    accumulatedWords <- accumulatedWords + descendingFrequencies[[index]]
    lastIndex <- index
    
    if (accumulatedWords >= coveragePercentage) break 
  }

  lastIndex
}

```
Elegiblity done ## demoral

```{r non-english-words, echo=TRUE}

# Return a data frame with 2 column, word and valid (TRUE for words in English, FALSE otherwise)
detectNonEnglishWords <- function(line) {
  
  convertWord <- function(word) iconv(word, 'ISO8859-1', 'ASCII', sub = '<NON_ENGLISH_LETTER>')
  
  isNotConvertedWord <- function(word) !str_detect(convertWord(word), '<NON_ENGLISH_LETTER>')
  
  wordsInLine <- str_split(line, boundary("word"))[[1]]
  wordsDF <- data.frame(word = wordsInLine)
  wordsDF <- wordsDF %>% 
    rowwise() %>% 
    mutate(valid = isNotConvertedWord(word))
  
  wordsDF
}
```
An example applying text ***'The Fußball is the King of Sports'*** (using ***Fußball*** in German instead of ***Football*** in English)
```{r non-english-words-2, echo=TRUE}
originalText <- 'The Fußball is the King of Sports'
originalText

detectNonEnglishWords('The Fußball is the King of Sports')
```
This function can be used for removing non-english words as well:
```{r non-english-words-3, echo=TRUE}

# Remove non-english words from a line of text
removeNonEnglishWords <- function(line) {
  wordsDF <- detectNonEnglishWords(line)
  filteredLine <- paste(wordsDF[wordsDF$valid == TRUE, 'word']$word, collapse = " ")
  filteredLine
}

originalText <- 'The Fußball is the King of Sports'
originalText

removeNonEnglishWords('The Fußball is the King of Sports')
```


### 3.2) Further steps
The next steps of the project will be to build a predictive algorithm using N-Grams lookups, in order to compute probabilites for the next occurence regarding to the previous words, backing off to a lower level (e.g. from 3-gram to 2-gram, and so forth) as needed. Later, developing a web app (using [***Shiny***](https://shiny.rstudio.com/)) that uses such algorithm, suggesting to the user the next word.

## Apendix I - Source codes

This document has been generated using [***R Mardown***](http://rmarkdown.rstudio.com/). Its ***.Rmd*** source code that can be found at: <https://github.com/laplata2003/data-science-capstone-week2-milestone-report>.

